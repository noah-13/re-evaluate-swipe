{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYSD2M4m6DtQ",
        "outputId": "49d1993a-5186-4800-8c3c-8980d7f8b90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'referee'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 40 (delta 4), reused 27 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (40/40), 35.04 KiB | 588.00 KiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/i-need-sleep/referee.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd referee/code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2sy0QKH6Jy_",
        "outputId": "afeb4652-3414-4ac1-dedd-b524164770c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/referee/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoSOg1eJ6TLq",
        "outputId": "d7a03b6c-10a7-4f2c-e740-5cf1db789bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting transformers==4.32.0 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->-r requirements.txt (line 1)) (12.8.61)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.32.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.32.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.32.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.32.0->-r requirements.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, transformers, torch\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.32.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.18.1 tokenizers-0.13.3 torch-2.1.0 transformers-4.32.0 triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnysUvP0ygwi",
        "outputId": "b31fe542-fb19-482f-b1dc-c8132309d8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/referee/code/utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python fetch_huggingface_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADgy4mJI6rLT",
        "outputId": "dd62e7df-4104-4eda-ba41-723ba212eedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 206kB/s]\n",
            "config.json: 100% 579/579 [00:00<00:00, 2.70MB/s]\n",
            "tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 221kB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 40.2MB/s]\n",
            "tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 263kB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 579/579 [00:00<00:00, 3.80MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin: 100% 371M/371M [00:01<00:00, 238MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ_5l-Hmz20D",
        "outputId": "6aae7f68-4198-42ef-92bc-0bd8abaadaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/referee/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from models.deberta_for_eval import DebertaForEval\n",
        "import utils.globals as uglobals\n",
        "import os\n",
        "\n",
        "# Load CSV file\n",
        "csv_filename = \"input.csv\"\n",
        "df = pd.read_csv(csv_filename)\n",
        "\n",
        "# Define model names\n",
        "model_names = [\n",
        "    'access',\n",
        "    'keep_it_simple',\n",
        "    'swipe_v5',\n",
        "    'swipe_v5_clean',\n",
        "    'gpt3',\n",
        "    'bartl_wikilarge'\n",
        "]\n",
        "\n",
        "# Set sample size for testing\n",
        "df_sample = df.copy()\n",
        "\n",
        "# Initialize the Referee model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DebertaForEval(uglobals.DERBERTA_MODEL_DIR, uglobals.DERBERTA_TOKENIZER_DIR, device, head_type='linear')\n",
        "model.load_state_dict(torch.load(\"pretrained_deberta.bin\", map_location=device)['model_state_dict'], strict=False)\n",
        "model.eval()\n",
        "\n",
        "# Function to compute Referee scores\n",
        "def compute_referee_score(pairs):\n",
        "    scores = []\n",
        "    for pair in pairs:\n",
        "        model_input = [str(pair[0]) + ' ' + model.tokenizer.sep_token + ' ' + str(pair[1])]\n",
        "        model_out = model(model_input)\n",
        "        scores.append(model_out[:, -1].item())  # Extract Referee score\n",
        "    return scores\n",
        "\n",
        "# Compute Referee scores for each model\n",
        "source_texts = df_sample['r_content'].tolist()\n",
        "for name in model_names:\n",
        "    hypotheses = df_sample['candidate_' + name].tolist()\n",
        "\n",
        "    # Create input pairs [[source, hypothesis], ...]\n",
        "    input_pairs = list(zip(source_texts, hypotheses))\n",
        "\n",
        "    # Compute scores\n",
        "    df_sample.loc[:, \"referee_score_\" + name] = compute_referee_score(input_pairs)\n",
        "# Compute Referee scores for reference texts\n",
        "reference_texts = df_sample['s_content'].tolist()\n",
        "reference_pairs = list(zip(source_texts, reference_texts))\n",
        "df_sample.loc[:, \"referee_score_reference\"] = compute_referee_score(reference_pairs)\n",
        "\n",
        "# Save the updated sample CSV file\n",
        "new_csv_filename = \"referee.csv\"\n",
        "df_sample.to_csv(new_csv_filename, index=False)\n",
        "\n",
        "print(f\"CSV file has been updated and saved as: {new_csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQghHwqB6xUs",
        "outputId": "9f377833-b9cf-49e3-b658-e3f7302b0573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample CSV file has been updated and saved as: test_model_responses_referee.csv\n"
          ]
        }
      ]
    }
  ]
}